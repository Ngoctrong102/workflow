springdoc:
  api-docs:
    path: /api-docs
  swagger-ui:
    path: /swagger-ui.html
    enabled: true

spring:
  application:
    name: notification-platform
  main:
    allow-circular-references: true
  
  datasource:
    url: jdbc:postgresql://localhost:5432/notification_platform
    username: ${DB_USERNAME:postgres}
    password: ${DB_PASSWORD:postgres}
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
      leak-detection-threshold: 60000
      pool-name: NotificationPlatformPool
  
  jpa:
    hibernate:
      # Use 'update' to automatically update schema on startup (adds new tables/columns, keeps existing data)
      # Use 'create-drop' to drop and recreate schema on startup/shutdown (WARNING: deletes all data)
      # Use 'create' to create schema on startup (drops existing schema first)
      # Use 'none' to disable automatic schema management
      ddl-auto: ${JPA_DDL_AUTO:update}
    show-sql: ${JPA_SHOW_SQL:false}
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: true
        use_sql_comments: true
        type:
          json_format_mapper: jackson
        # Enable automatic schema export and updates
        jdbc:
          lob:
            non_contextual_creation: true
        # Allow Hibernate to manage schema changes
        physical_naming_strategy: org.hibernate.boot.model.naming.CamelCaseToUnderscoresNamingStrategy
  
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    consumer:
      group-id: notification-platform-consumer
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
  
  rabbitmq:
    host: ${RABBITMQ_HOST:localhost}
    port: ${RABBITMQ_PORT:5672}
    username: ${RABBITMQ_USERNAME:guest}
    password: ${RABBITMQ_PASSWORD:guest}
    virtual-host: ${RABBITMQ_VIRTUAL_HOST:/}
    listener:
      simple:
        acknowledge-mode: manual
        prefetch: 10
        concurrency: 1
        max-concurrency: 10
    queues: ${RABBITMQ_QUEUES:events}
  
  redis:
    host: ${REDIS_HOST:localhost}
    port: ${REDIS_PORT:6379}
    password: ${REDIS_PASSWORD:}
    database: ${REDIS_DATABASE:0}
    timeout: 2000ms
    lettuce:
      pool:
        max-active: 8
        max-idle: 8
        min-idle: 0

server:
  port: ${SERVER_PORT:8080}
  servlet:
    context-path: /api/v1

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
      probes:
        enabled: true
  health:
    db:
      enabled: true
    kafka:
      enabled: true

rate-limiting:
  window-type: ${RATE_LIMIT_WINDOW_TYPE:minute} # minute, hour, day
  default-limit: ${RATE_LIMIT_DEFAULT:100} # Default requests per window
  enabled: ${RATE_LIMIT_ENABLED:true}

dlq:
  max-retries: ${DLQ_MAX_RETRIES:3}
  cleanup-days: ${DLQ_CLEANUP_DAYS:30} # Days to keep resolved/failed entries
  auto-retry: ${DLQ_AUTO_RETRY:false} # Auto retry pending entries

app:
  data-cleanup:
    enabled: ${DATA_CLEANUP_ENABLED:true}
    retention-months: ${DATA_RETENTION_MONTHS:6} # Keep data for 6 months

logging:
  level:
    root: INFO
    com.notificationplatform: DEBUG
    org.springframework.web: INFO
    org.hibernate.SQL: DEBUG
    org.hibernate.type.descriptor.sql.BasicBinder: TRACE
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"

